{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing, building a Pandas dataframe and saving it as a  .csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69\n",
      "621\n",
      "333\n",
      "38\n",
      "105\n",
      "105\n",
      "<bound method NDFrame.head of                                                   body replier thread_no\n",
      "0    [person] notice dbg package kernel move not ap...       0         9\n",
      "1     Quoting Riku Voipio [number] Thanks Philip ra...       1        11\n",
      "2     Quoting Riku Voipio [number] Thanks Philip ra...       2        11\n",
      "3     On Sat [number] + Ole Streicher write You mis...       3        13\n",
      "4     On Sat [number] + Ole Streicher write You mis...       4        13\n",
      "5     On Sat [number] + Ole Streicher write You mis...       4        13\n",
      "6     On Sat [number] + Ole Streicher write You mis...       5        13\n",
      "7     On Sat [number] + Ole Streicher write You mis...       4        13\n",
      "8     On Sat [number] + Ole Streicher write You mis...       6        13\n",
      "9     On Sat [number] + Ole Streicher write You mis...       5        13\n",
      "10    On Sat [number] + Ole Streicher write You mis...       7        13\n",
      "11    On Sat [number] + Ole Streicher write You mis...       3        13\n",
      "12    On Sat [number] + Ole Streicher write You mis...       8        13\n",
      "13    On Sat [number] + Ole Streicher write You mis...       8        13\n",
      "14    On Sat [number] + Ole Streicher write You mis...       9        13\n",
      "15    On Sat [number] + Ole Streicher write You mis...       9        13\n",
      "16    On Sat [number] + Ole Streicher write You mis...      10        13\n",
      "17    On Sat [number] + Ole Streicher write You mis...      11        13\n",
      "18    On Sat [number] + Ole Streicher write You mis...      12        13\n",
      "19    On Sat [number] + Ole Streicher write You mis...      13        13\n",
      "20    On Sat [number] + Ole Streicher write You mis...      14        13\n",
      "21    On Sat [number] + Ole Streicher write You mis...       7        13\n",
      "22    On Sat [number] + Ole Streicher write You mis...      15        13\n",
      "23    On Sat [number] + Ole Streicher write You mis...       9        13\n",
      "24    On Sat [number] + Ole Streicher write You mis...       9        13\n",
      "25    On Sat [number] + Ole Streicher write You mis...       8        13\n",
      "26    On Sat [number] + Ole Streicher write You mis...      16        13\n",
      "27    On Sat [number] + Ole Streicher write You mis...       9        13\n",
      "28    On Sat [number] + Ole Streicher write You mis...      16        13\n",
      "29    On Sat [number] + Ole Streicher write You mis...       7        13\n",
      "..                                                 ...     ...       ...\n",
      "303   Oi lot[person] wonder[person]ll switch word d...      28       216\n",
      "304   Oi lot[person] wonder[person]ll switch word d...      28       216\n",
      "305   Oi lot[person] wonder[person]ll switch word d...      70       216\n",
      "306   Oi lot[person] wonder[person]ll switch word d...      77       216\n",
      "307   Oi lot[person] wonder[person]ll switch word d...     101       216\n",
      "308   Package wnpp Severity wishlist Owner Andreas ...     102       217\n",
      "309   Package wnpp Severity wishlist Owner Andreas ...     102       217\n",
      "310   Package wnpp Severity wishlist Owner Andreas ...     102       217\n",
      "311   Package wnpp Severity wishlist Owner Andreas ...      40       217\n",
      "312   Package wnpp Severity wishlist Owner Andreas ...       8       217\n",
      "313   Package wnpp Severity wishlist Owner Andreas ...     102       217\n",
      "314   Package wnpp Severity wishlist Owner Andreas ...     102       217\n",
      "315   Package wnpp Severity wishlist Owner Andreas ...       8       217\n",
      "316   Package wnpp Severity wishlist Owner Andreas ...       8       217\n",
      "317   Package wnpp Severity wishlist Owner Andreas ...       8       217\n",
      "318   Package wnpp Severity wishlist Owner Andreas ...     102       217\n",
      "319   Sean Whitton write Re Converting dgit How his...      33       227\n",
      "320   Sean Whitton write Re Converting dgit How his...      17       227\n",
      "321   Quoting Vijeth T Aradhya [number] Great You[m...      24       233\n",
      "322   Quoting Vijeth T Aradhya [number] Great You[m...      22       233\n",
      "323   Package wnpp Severity wishlist Owner akash < ...      36       234\n",
      "324   On [number] Christoph Biedl write fyi[person]...      68       242\n",
      "325   Dear powerpcspe sh[number] buildd maintainer ...      54       246\n",
      "326   Dear powerpcspe sh[number] buildd maintainer ...      83       246\n",
      "327   Dear powerpcspe sh[number] buildd maintainer ...      54       246\n",
      "328   Dear powerpcspe sh[number] buildd maintainer ...      83       246\n",
      "329   Hi[person]s work port Debian run browser JS V...     103       255\n",
      "330   Hi[person]s work port Debian run browser JS V...      24       255\n",
      "331   Hi[person]s work port Debian run browser JS V...     104       255\n",
      "332   Hi[person]s work port Debian run browser JS V...     103       255\n",
      "\n",
      "[333 rows x 3 columns]>\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import sys\n",
    "import glob\n",
    "import string\n",
    "from pprint import pprint\n",
    "from collections import Counter, OrderedDict\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en',disable=['parser', 'tagger', 'ner'])\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from tqdm import tqdm, tqdm_notebook, tnrange\n",
    "tqdm.pandas(desc='Progress')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity='all'\n",
    "\n",
    "folder_path = \"/home/niki/Downloads/Deb/Thread/*\"\n",
    "file_name = \"/home/niki/Documents/BE_Project/my_EmailRecommmendation/model/dataframe2.csv\"\n",
    "sys.path.insert(0, '/home/niki/Documents/BE_Project/my_EmailRecommmendation/Preprocessing')\n",
    "\n",
    "\n",
    "import preprocessing\n",
    "\n",
    "def get_sender(msg):\n",
    "    msg = email.message_from_string(msg)\n",
    "    mfrom = msg['From'].split('<')[0]\n",
    "    return mfrom\n",
    "\n",
    "def extract_debian(text):\n",
    "    text = text.split('\\n\\n\\n')\n",
    "    header = text[0].split('\\n')\n",
    "    body = text[1]\n",
    "    sender = header[2].split(':')[1].split('<')[0]\n",
    "#     print('Sender',sender)\n",
    "#     print('Body \\n',body)\n",
    "    return sender,body\n",
    "\n",
    "def clean_debian(temp):\n",
    "    temp = re.sub('\\n+','\\n',temp)\n",
    "    temp = re.sub('\\n',' ',temp)\n",
    "    temp = re.sub('\\t',' ',temp)\n",
    "    temp = re.sub(' +',' ',temp)\n",
    "    return temp\n",
    "\n",
    "def deb_lemmatize(doc):        \n",
    "    doc = nlp(doc)\n",
    "    article, skl_texts = '',''\n",
    "    for w in doc:\n",
    "        if w.text != '\\n' and not w.is_stop and not w.is_punct and not w.like_num:\n",
    "            article += \" \" + w.lemma_\n",
    "        if w.text == '\\n':                \n",
    "            skl_texts += \" \" + article\n",
    "            article = ''       \n",
    "    return skl_texts\n",
    "\n",
    "def deb_toppostremoval(temp):\n",
    "    strings = temp.splitlines()\n",
    "    temp = ''\n",
    "    for st in strings:\n",
    "        st = st.strip()\n",
    "        if len(st)>0:\n",
    "            if st[0]=='>':\n",
    "                continue\n",
    "            else:\n",
    "                temp += '\\n' + st\n",
    "    return temp\n",
    "\n",
    "df = pd.DataFrame(columns=['body','replier', 'thread_no'])\n",
    "users = []\n",
    "folder = glob.glob(folder_path)\n",
    "th_no = 0\n",
    "obj = preprocessing.preprocess()\n",
    "cnt = 0\n",
    "count_file = 0\n",
    "for fol in folder:\n",
    "    files = glob.glob(fol+'/*.txt')\n",
    "    flag = 0\n",
    "    t = ''\n",
    "    for file in files:\n",
    "        count_file += 1\n",
    "        dataa = open(file,'r')\n",
    "        temp = dataa.read()\n",
    "        sender,temp = extract_debian(temp)\n",
    "        users.append(sender)\n",
    "        temp = deb_toppostremoval(temp)\n",
    "        temp = deb_lemmatize(temp)\n",
    "        temp = clean_debian(temp)\n",
    "        if temp == '':\n",
    "            cnt += 1\n",
    "            continue\n",
    "        temp = obj.replace_tokens(temp)\n",
    "        if flag==0:\n",
    "            t = temp\n",
    "            flag = 1\n",
    "            continue\n",
    "        df = df.append({'body': str(t),'replier':sender, 'thread_no':th_no}, ignore_index=True)\n",
    "        t = t + temp\n",
    "        dataa.close()\n",
    "    th_no += 1\n",
    "print(cnt)\n",
    "print(count_file)\n",
    "print(len(df['body']))\n",
    "print(len(df['thread_no'].unique()))\n",
    "print(len(df['replier'].unique()))\n",
    "rep_to_index = {}\n",
    "count = 0\n",
    "for rep in df['replier']:\n",
    "    if rep in rep_to_index:\n",
    "        continue\n",
    "    else:\n",
    "        rep_to_index[rep] = count\n",
    "        count += 1\n",
    "pprint(len(rep_to_index))\n",
    "for rep in df['replier']:\n",
    "    df.loc[df['replier']==rep,'replier'] = rep_to_index[rep]\n",
    "print(df.head)\n",
    "df.to_csv(file_name)\n",
    "unique_users = len(df.replier.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexing of words in vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = Counter()\n",
    "for sent in df.body.values:\n",
    "    words.update(w.text.lower() for w in nlp(sent))\n",
    "\n",
    "words = sorted(words, key=words.get, reverse=True)\n",
    "words = ['_PAD','_UNK'] + words\n",
    "\n",
    "word2idx = {o:i for i,o in enumerate(words)}\n",
    "idx2word = {i:o for i,o in enumerate(words)}\n",
    "def indexer(s): return [word2idx[w.text.lower()] for w in nlp(s)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorizeData(Dataset):\n",
    "    def __init__(self, df_path, maxlen=10):\n",
    "        self.df = pd.read_csv(df_path, error_bad_lines=False)\n",
    "        self.df['body'] = self.df.body.apply(lambda x: x.strip())\n",
    "        print('Indexing...')\n",
    "        self.df['bodyidx'] = self.df.body.apply(indexer)\n",
    "        print('Calculating lengths')\n",
    "        self.df['lengths'] = self.df.bodyidx.apply(len)\n",
    "        self.maxlen = max(self.df['lengths'])\n",
    "        print('Padding')\n",
    "        self.df['bodypadded'] = self.df.bodyidx.apply(self.pad_data)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        X = self.df.bodypadded[idx]\n",
    "        lens = self.df.lengths[idx]\n",
    "        y = self.df.replier[idx]\n",
    "        return X,y,lens\n",
    "    \n",
    "    def pad_data(self, s):\n",
    "        padded = np.zeros((self.maxlen,), dtype=np.int64)\n",
    "        if len(s) > self.maxlen: padded[:] = s[:self.maxlen]\n",
    "        else: padded[:len(s)] = s\n",
    "        return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing...\n",
      "Calculating lengths\n",
      "Padding\n"
     ]
    }
   ],
   "source": [
    "ds = VectorizeData(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = ds.maxlen\n",
    "hidden_size = 30\n",
    "num_classes = unique_users\n",
    "num_epochs = 5\n",
    "batch_size = 2\n",
    "learning_rate = 0.001\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size) \n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)  \n",
    "    \n",
    "    def forward(self, x):\n",
    "#         x = torch.FloatTensor(x)\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNet(input_size, hidden_size, num_classes).to(device)\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "opt = torch.optim.Adam(model.parameters(), lr=learning_rate)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c76655e8d4324a08b86c9437c1c50172",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=333), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " Epoch 0: Train loss: -3053831.75 acc: 0.09309309309309309\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a2d22a4f34943239810f0086723f62b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=333), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dl= DataLoader(ds, batch_size=1)\n",
    "num_batch = len(train_dl)\n",
    "for epoch in range(num_epochs):\n",
    "    y_true_train = list()\n",
    "    y_pred_train = list()\n",
    "    total_loss_train = 0\n",
    "    t = tqdm_notebook(iter(train_dl), leave=False, total=num_batch)\n",
    "    for X,y, lengths in t:\n",
    "    #     X = X.transpose(0,1)\n",
    "        X = Variable(X.cpu())\n",
    "        y = Variable(y.cpu())\n",
    "        lengths = lengths.numpy()\n",
    "\n",
    "        opt.zero_grad()\n",
    "        X = X.float()\n",
    "        pred = model(X)\n",
    "        loss = F.nll_loss(pred, y)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        t.set_postfix(loss=loss.data[0])\n",
    "        pred_idx = torch.max(pred, dim=1)[1]\n",
    "\n",
    "        y_true_train += list(y.cpu().data.numpy())\n",
    "        y_pred_train += list(pred_idx.cpu().data.numpy())\n",
    "        total_loss_train += loss.data[0]\n",
    "\n",
    "    train_acc = accuracy_score(y_true_train, y_pred_train)\n",
    "    train_loss = total_loss_train/len(train_dl)\n",
    "    print(f' Epoch {epoch}: Train loss: {train_loss} acc: {train_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
